---
author: "patrick.unicorn"
output:
  html_document:
    css: ../../css/main.css
    highlight: null
    theme: null
---

读《Java Performance: The Definitive Guide》

* * *

## 基本概念

+ JVM 性能调优的过程实际上与 C++ 程序员在编译时通过测试选择编译参数，以及 PHP 码农在 php.ini
文件中选择适当变量等过程非常类似。

+ 编码和调优常常被认为是两个不同的专业领域：性能调优工程师只是竭力将 JVM的性能发挥到极致，而开发人员只关心他们的代码逻辑是否正确。这种区分没有什么意义。任何从事Java相关工作的人都应该熟谙代码在 JVM 中的行为，以及如何调优才能提升性能。

+ 商业版 JVM 中和性能密切相关的一个特性就是 Java 飞行记录器（Java Flight Recorder，JFR）。

+ 除了少数例外，JVM 主要接受两类标志：布尔标志和附带参数的标志。

+ 布尔标志采用以下语法： -XX:+FlagName 表示开启， -XX:-FlagName 表示关闭。

+ 附带参数的标志采用以下语法： -XX:FlagName=something ，表示将标志 flagName 的值设置为 something 。其中 something 通常可以为任意值。例如 -XX:NewRatio=N ，表示 NewRatio可以设置为任意值 N。

+ 在给定的命令行上，添加 -XX:+Printflagsfinal （默认为 false ，即“关闭” ）就能获得具体运行环境中特定标志的默认值。

+ 需要更高性能时，算法是否优秀就是重中之重了。

+ 要创建和销毁的对象越多，垃圾收集的工作量就越大；要分配和持有的对象越多，GC 的周期就越长；要从磁盘装载进 JVM 的类越
多，程序启动所花费的时间就越长；要执行的代码越多，机器硬件缓存的效率就越低；而执行的代码越多，花费的时间就越长。

+ 随着新特性的添加和新要求的采纳（为了与对手竞争） ，程序会越来越大，越来越慢。

+ “我们不应该把大量时间都耗费在那些小的性能改进上；过早考虑优化是所有噩梦的根源” 。这句名言的重点是，最终你应该编写清晰、直接、易读和易理解的代码。这里的“优化”应该理解为虽然算法和设计改变了复杂程序的结构，但是提供了更好的性能。那些真正的优化最好留到以后，等到性能分析表明这些措施有巨大收益的时候才进行。而这里所指的过早优化，并不包括避免那些已经知道对性能不好的代码结构。每行代码，如果有两种简单、直接的编程方式，那就应该选择性能更好的那种。

+ 常用的打印日志中，如果有拼接字符串的消息，应该首先判断日志是否开启，以避免不必要的字符串连接开销，比如：

```
// 不好
log.log(Level.FINE, "I am here, and the value of X is" 
    + calcX() + " and Y is " + calcY());
    
// 优化
if (log.isLoggable(Level.FINE)) {
    log.log(Level.FINE,
        "I am here, and the value of X is {} and Y is {}",
        new Object[]{calcX(), calcY()});
}
```

+ 在分布式环境中，比如 Java EE 应用服务器、负载均衡器、数据库和后台企业信息系统，Java 应用服务
器的性能问题可能只是其中很小的部分。

+ 对于整体系统，我们需要采取结构化方法针对系统的所有方面分析性能。CPU使用率、I/O延迟、系统整体的吞吐量都必须测量和分析。只有到那时，我们才能判定到底是哪个组件导致了性能瓶颈。

+ 增加系统某个组件的负载从而导致整个系统性能变慢，这项原则不仅限于数据库。CPU密集型的应用服务器增加负载，或者越来越多线程试图获取已经有线程等待的锁，还有许多其他场景，也都适用这项原则。

+ 优化的原则：

<blockquote>
+ 借助性能分析来优化代码，重点关注性能分析中最耗时的操作。然而请注意，这并不意
味着只看性能分析中的叶子方法 。

+ 利用奥卡姆剃刀原则诊断性能问题。性能问题最可能的原因应该是最容易解释的：新代
码比机器配置更可能引入性能问题，而机器配置比 JVM 或者操作系统的 bug 更容易引
入性能问题。隐藏的 bug 确实存在，但不应该把最可能引起性能问题的原因首先归咎于
它，而只在测试用例通过某种方式触发了隐藏的 bug 时才关注。但不应该一上来就跳到
这种不太可能的场景。

+ 为应用中最常用的操作编写简单算法。以估算数学公式的程序为例，用户可以决定他所
期望的最大容许误差为 10% 或 1%。如果 10% 的误差适合多数用户，那么优化代码就
意味着即便误差范围缩小为 1%，但是速度变慢了
</blockquote>

+ **微基准测试**用来测量微小代码单元的性能，包括调用同步方法的用时与非同步方法的用时比较，创建线程的代价与使用线程池的代价，执行某种算法的耗时与其替代实现的耗时，等等。相当于测试函数或代码片段。

+ 编写多线程微基准测试时务必深思熟虑。当若干个线程同时执行小段代码时，极有可能会产生同步瓶颈（以及其他线程问题） 。所以，如果我们过多依赖多线程基准测试的结果，就常常会将大量时间花费在优化那些真实场景中很少出现的同步瓶颈上，而不是性能需求更迫切的地方。

+ 考虑这样的微基准测试，即有两个线程同时调用同步方法。由于基准测试的代码量相对于被测方法来说比较少，所以多数时间都是在执行同步方法。假设执行同步方法的时间只占整个微基准测试的 50%，即便少到只有两个线程，同时执行同步代码的概率仍然很高。因此基准测试运行得很慢，并且随着线程数的增加，竞争所导致的性能问题将愈演愈烈。最终结果就是，测试衡量的是 JVM 如何处理竞争，而不是微基准测试的本来目的。

+ 微基准测试中的输入值必须事先计算好，不能使用随机数生成函数放在测试代码中，因为，随机生成函数也要耗费时间。

+ 我们应该为常见的场景进行优化，即使需要加入特殊的处理逻辑。

+ Java 的一个特点就是代码执行的越多性能越好，基于这点，微基准测试应该包括热身期，使得编译器能生成优化的代码。

+ 在做回归测试的时候，追踪级别设为纳秒很有意义。如果集合操作每次都节约几纳秒，日积月累下来意义就很重大了。对于那些不频繁的操作来说，例如那种同时只需处理一个请求的 servlet，修复微基
准测试所发现的纳秒级性能衰减就是浪费时间，这些时间用在优化其他操作上可能会更有价值。微基准测试难于编写，真正管用的又很有限。所以，应该了解这些相关的隐患后再做出决定，是微基准测试合情合理值得做，还是关注宏观的测试更好。

+ 衡量应用性能最好的事物就是应用自身，以及它所用到的外部资源。如果正常情况下应用需要调用 LDAP 来检验用户凭证，那应用就应该在这种模式下测试。虽然删空 LDAP 调用在模块测试中有一定意义，但应用本身必须在完整真实配置的环境中测试。

+ 任何应用（包含独立运行的 JVM）都可以像这样划分成一系列步骤，方框中的模块、子系统等产生数据的速度取决于它们的效率。（耗费的时间包括子系统代码的执行时间，也包括网络传输的时间、磁盘传输的时间，等等。如果是模块化的模型，时间应该只包括该模块内代码的执行时间。）数据进入子系统的速率取决于前一个模块或系统的输出速率。

+ 全应用测试有个很重要的场景，就是同一台机器上同时运行多个应用。许多 JVM 的默认调优都是默认假定整个机器的资源都归 JVM 使用。如果单独测试，优化效果很好。如果在其他应用（包括但不限于 Java 程序）运行的时候进行测试，性能会有很大的不同。

+ 单个 JVM（默认配置）执行 GC 周期时，该机器上所有处理器的 CPU 使用率都会变成 100%。如果测量程序执行时的平均CPU 使用率，大概会有 40%——实际意思是，某些时候 30%的CPU被占用，其他时候为100%。当隔离 JVM 时，它可以运行得很好，但如果 JVM 与其他应用并发运行，它就不可能在 GC 时获得 100% 的CPU。此时测出来的性能会与它单独运行时不同。这是微基准测试和模块测试不可能让你全面了解应用性能的另一个原因。

+ 不进行整体应用的测试，就不可能知道哪部分的优化会产生回报。

+ 使 Java 代码更快的常用方法是更好的算法，这不依赖Java 调优或者 Java 编码实践。

+ 虚拟机会花几分钟（或更长时间）全面优化代码并以最高性能执行。由于这个（以及其他）原因，研究Java的性能优化就要密切注意代码优化的热身期：大多数时候，应该在运行代码执行足够长时间，已经编译并优化之后再测量性能。

+ 许多情况下应用从开始到结束的整体性能更为重要。报告生成器处理 10 000 个数据元素需要花费大量时间，但对最终用户而言，处理前 5000个元素是否比后5000个慢50%并不重要。即便是像应用服务器这样的系统——其性能必定会随运行时间而改善——初始的性能依然很重要。某种配置下的应用服务器需要 45 分钟才能达到性能峰值。

+ 在客户端 - 服务器的吞吐量测试中，并不考虑客户端的思考时间。客户端向服务器发送请求，当它收到响应时，立刻发送新的请求。持续这样的过程，等到测试结束时，客户端会报告它所完成的操作总量。客户端常常有多个线处理，所以吞吐量就是所有客户端所完成的操作总量。通常这个数字就是每秒完成的操作量，而不是测量期间的总操作量。这个指标常常被称作每秒事务数（TPS） 、每秒请求数（RPS）或每秒操作次数（OPS） 。

+ 响应时间测试和吞吐量测试（假设后者是基于客户端 - 服务器模式）之间的差别是，响应时间测试中的客户端线程会在操作之间休眠一段时间。这被称为思考时间。响应时间测试是尽量模拟用户行为：用户在浏览器输入URL，用一些时间阅读返回的网页，然后点击页面上的链接，花一些时间阅读返回的网页，等等。当测试中引入思考时间时，吞吐量就固定了：指定数量的客户端，在给定思考时间下总是得到相同的 TPS（少许差别，参见框注） 。基于这点，测量请求的响应时间就变得重要了：服务器的效率取决于它响应固定负载有多快。

+ 性能测试的结果会随时间而变。即便程序每次处理的数据集都相同，产生的结果也仍然会有差别。因为有很多因素会影响程序的运行，如机器上的后台进程，网络时不时的拥堵等。好的基准测试不会每次都处理相同的数据集，而是会在测试中制造一些随机行为以模拟真实的世界。这就会带来一个问题：运行结果之间的差别，到底是因为性能有变化，还是因为测试的随机性。可用以下方法来解决这个问题，即多次运行测试，然后对结果求平均。当被测代码发生变化时，就再多次运行测试，对结果求平均，然后比较两个平均值。

+ 比较基准测试的结果时，我们不可能知道平均值的差异是真的性能有差还是随机涨落。最好的办法是先假设“平均值是一样的” ，然后确定该命题为真时的几率。如果命题很大几率为假，我们就有信心认为平均值是真的有差别（虽然我们永远无法 100% 肯定） 。注：《测试技术计量》课程，有一些对实验结果进行比对的方法。

+ 因代码更改而进行的测试称为回归测试。在回归测试中，原先的代码称为基线（baseline） ，而新的代码称为试样（specimen） 。

+ 所有的客户端 - 服务器测试都存在风险，即客户端不能足够快地向服务器发送数据。这可能是由于客户端机器的 CPU 不足以支持所需数量的客户端线程，也可能是因为客户端需要花大量时间处理响应才能发送新的请求。在这些场景中，测试衡量的其实是客户端性能而不是服务器性能的。其中的风险依赖于每个线程所承载的工作量（客户端机器的线程数和配置） 。由于客户端线程需要执行大量工作，零思考时间（面向吞吐量）测试更可能会遇到这种情形。因此，通常吞吐量测试比响应时间测试的线程数少，线程负载也小。通常吞吐量测试也会报告请求的平均响应时间。但平均响应时间的变化并不表示性能有问题，除非报告的吞吐量相同。能够承受 500 OPS、响应时间 0.5 秒的服务器，它的性能要好过响应时间 0.3 秒但只有 400 OPS 的服务器。吞吐量测试总是在合适的热身期之后进行。

+ 衡量响应时间有两种方法。响应时间可以报告为平均值：请求时间的总和除以请求数。响应时间也可以报告为百分位请求，例如第 90 百分位响应时间。如果 90% 的请求响应小于1.5 秒，且 10% 的请求响应不小于 1.5 秒，则 1.5 秒就是第 90 百分位响应时间。两种方法的一个区别在于，平均值会受离群值影响。这是因为计算平均值时包括了离群值。离群值越大，对平均响应时间的影响就会越大。对于Java应用来说，由于GC的存在，更容易引起这样的离群值。因此，一般最好同时考虑**平均响应时间**和至少一种**百分位响应时间**。

+  Faban（http://faban.org/）为例，这是一个开源的、基于 Java 的负载生成器。

+ 查找代码性能变化的正确方法是先决定一个显著性水平——比如 0.1——然后用 t 检验判定在这个显著性水平上试样是否与基线有差别。

```
统计学中的显著性与重要性

显著性差异并不意味着统计结果对我们更重要。平均为 1 秒的变化很小的基线，和平均为 1.01 秒的变化很小的试样，其 p 值可能为 0.01：结果的差别有 99% 的置信度。
但结果的差别只有 1%。现在假定另外一个测试，试样和基线有 10% 的变动，但是 p值为 0.2，即非统计显著。哪个测试的结果最为重要？这需要更多时间来审查。审查后发现虽然相差 10% 的测试的置信度低，但在用时上更加优化（如果可能的话，可以用更多数据来验证测试结果是否真的统计显著） 。仅仅因为 1% 差异的可能性更大，并不意味着它更重要。
```

+  Apache Commons Mathematics 类库中的 TTest 可以计算统计结果的**置信度**。

+ 程序时不时会表现出随机行为，而一旦引入了随机性，我们就再也无法 100% 确定这些数据意味着什么了。使用统计分析有助于使结果变得更客观，但即便如此，仍然免不了主观臆断。理解这些数据背后的概率及其意义，有助于降低主观性。

## 性能调优工具

+ 通常 CPU 使用率可以分为两类：用户态时间和系统态时间（Windows 上被称作 privileged time） 。用户态时间是 CPU 执行应用代码所占时间的百分比，而系统态时间则是 CPU 执行内核代码所占时间的百分比。系统态时间与应用相关，比如应用执行 I/O 操作，系统就会执行内核代码从磁盘读取文件，或者将缓冲数据发送到网络，等等。任何使用底层系统资源的操作，都会导致应用占用更多的系统态时间。性能调优的目的是，在尽可能短的时间内让 CPU 使用率尽可能地高。

+ CPU 使用率是一段时间内的平均数——5 秒、30 秒，也可能只有 1秒那么短（不过永远不会比这还要短） 。比如，10 分钟内一个程序执行的 CPU 使用率为50%。如果代码调优之后，CPU 使用率达到了 100%，说明程序的性能翻了倍：程序只需要执行 5 分钟就可以了。如果性能再翻倍，CPU 仍将是 100%，而执行完程序只要 2.5 分钟。CPU 使用率表示程序以多高的效率使用 CPU，所以数字越大，性能越好。

+ Linux查看CPU情况的工具是：**vmstat**。

+ 提高 CPU 使用率，一直都是批处理任务的目的。如果 CPU已经达到 100%，你仍然可以寻找优化，使得工作完成的更快（也要尽量保持 100%CPU 使
用率） 。

+ 一般来说，多 CPU 多线程的目的仍然是通过不阻塞线程来提高 CPU 使用率，或者是在线程完成工作等待更多任务时降低 CPU 使用率。

+ 在多线程多 CPU 下，需要重点考虑以下 CPU 空闲的情形：即便有事可做，CPU 仍然空闲。这在程序没有更多线程可用的时候可能会出现。典型的情况是，应用以固定尺寸
的线程池运行各种任务。每个线程同时只能执行一个任务，当线程被某个任务阻塞时（例如，等待数据库的响应） ，它就没法捡出新任务执行了。所以此时的情况就是，有任务需要执行（有事可做） ，却没有线程执行它们，结果就是 CPU 处在空闲时间。

+ Windows 和 Unix 系统都可以监控可运行（意味着没有被 I/O 阻塞、休眠等）的线程数。Unix 系统称之为运行队列（run queue） ，vmstat输出中就有。Windows 将这个数字称为处理器队列（processor quque），使用typeperf查看。但是需要注意：Unix 系统的运行队列长度（ vmstat 输出）是所有正在运行或待运行（即一旦有可用 CPU就可以运行）的线程数。而在 Windows 中，处理器队列长度就不包括正在运行的线程。

+ 如果试图运行的线程数超过了可用的 CPU，性能就会下降。一般来说，Windows 的处理器队列长度最好为 0，小于或等于 Unix 系统 CPU 的数目。如果在相当长时间内运行队列很长，说明系统已经过载，这时你应该检查系统，减少机器正在处理的工作量（将工作转移到其他机器或者优化代码） 。

+ 检查应用性能时，首先应该审查 CPU 时间。优化代码的目的是提升而不是降低（更短时间段内的）CPU 使用率。在试图深入优化应用前，应该先弄清楚为何 CPU 使用率低。

+ 监控磁盘使用率有两个目的。第一个目的与应用本身有关：如果应用正在做大量的磁盘I/O 操作，那 I/O 就很容易成为瓶颈。想了解何时磁盘 I/O 是瓶颈非常困难，因为这取决于应用的行为。

+ Linux系统使用**iostat**查看磁盘情况。

+ Linux 中可以查看进程的 47.89%的事件在 iowait （表示正在等待磁盘） ，这时表示磁盘速度赶不上IO请求了。

+ 监控磁盘使用率的第二个理由是——即便预计应用不会有很高的 I/O——有助于监控系统是否在进行内存交换。正在内存交换的系统——从主内存移动数据到磁盘或者反过来——一般来说，性能比较差。还有其他系统工具可以报告系统交换，例如 vmstat 输出中有两列（ si 是换进， so 是换出）可以警告我们系统是否正在交换。

+ 写入磁盘的应用遇到瓶颈，是因为写入数据的效率不高（吞吐量太低） ，或者是因为写入太多数据（吞吐量太高） 。

+ 网络使用率类似磁盘流量：应用可能没有充分利用网络所以带宽很低，或者写入某网络接口的总数据量超过了它所能处理的量。

+ 由于标准的系统工具通常只能显示某个网络接口发送和接收的数据报数和字节数，所以它们在监控网络流量方面差强人意。虽然这些信息有用，但无法告诉我们网络是没有充分利用，还是过度使用。

+ Unix 系统监控网络的基本工具是 netstat （大多数 Linux 发行版中还没有包括 netstat ，必须单独获得） 。Windows 上则可以在脚本中使用 typeperf ，监控网络使用率.

+ 有许多开源和商业工具可以监控网络带宽。Unix 里一个受欢迎的命令行工具就是 nicstat ，它可以显示每个网络接口的流量概要，包括网络接口的使用度。

+ typeperf 或 netstat 这样的工具可以报告读取和写入的数据，但是要计算网络使用率，你必须自己用脚本计算接口的带宽。虽然一般工具报告的单位是字节 / 秒（Bps） ，但请切记，带宽的单位是位 / 秒（bps） 。1000 兆位网络每秒处理 125 兆字节（MB） 。

+ 网络无法支持 100% 的使用率。对本地以太局域网来说，承受的网络使用率超过 40% 就意味着接口饱和了。如果网络是包交换或使用不同的传输介质，网络使用率的最大值就可能会不同，因此最好是评估网络架构之后再确定合适的值。这个值与 Java 无关，只是简单利用网络参数和操作系统接口。

+ 对基于网络的应用来说，务必要监控网络以确保它不是瓶颈。

+ `jcmd <process id> <command>` 用来打印Java进程的所涉及的基本类、线程和VM信息。

+ jconsole提供JVM活动的GUI视图。

+ jhat用于读取内存堆dump文件，用于分析。

+ jmap用于内存堆或其他JVM内存信息的dump。

+ jinfo查看JVM的系统属性，可以动态设置一些系统属性。

+ jstack转储Java进程中的线程栈信息，可以用于分析死锁的等。

+ jstat提供GC和类装载的信息。

+ jvm的GUI工具，可以用来剖析应用中的应用，也可以用来分析内存堆dump文件。

+ `jcmd process_id VM.uptime`或`jinfo -sysprops process_id`查看JVM运行多长时间了。

+ jconsole 的“VM 摘要”页可以显示程序所用的命令行，或者用 jcmd 显示`jcmd process_id VM.command_line`。

+ `jcmd process_id VM.flags [-all]`可以查看JVM的所有设置属性信息。类似与Gradle的`gradle properties`。

+ 诊断性能问题时，找出哪些标志起作用是很常见的事。JVM 运行时，可以用 jcmd 做到这一点。如果想找出特定 JVM 的平台特定的默认值是什么，那么在命令行上添加 -XX:+Printflagsfinal 会很有用：`java other_options -XX:+PrintFlagsFinal -version`，其与上面一项中的jcmd命令输出结果一样。

```
输出结果：uintx InitialHeapSize := 4169431040 {product / pd product / manageable /  C2 diagnostic}

1. 冒号表示标志使用的是非认值
2. product 表示在所有平台上的默认设置都是一致的。 
3. pd product 表示标志的默认值是独立于平台的。
4. manageable 表示运行时可以动态更改标志的值。
5. C2 diagnostic表示为编译器工程师提供诊断输出，帮助理解编译器正以什么方式运作
```

+ 另一种查看运行中的应用的此类信息的工具，叫作 jinfo 。 jinfo 的好处在于，它允许程序在执行时更改某个标志的值，并且可以单独检查输出单个标志的值。

```
jinfo 可以更改任意标志的值，但并不意味着 JVM 会响应更改。比如说，大多数影响 GC 算法行为的标志都在启动时使用，以决定垃圾收集器的行为方式。之后通过 jinfo 更改标志值，并不会导致 JVM 改变它的行为。它会以初始时的算法继续执行。所以这个技术只会对那些在 Printflagsfinal 输出中标记为 manageable 的标志有效
```

+ `jstack process_id`或者`jcmd process_id Thread.print`可以打印出每个线程的详细输出，对于分析线程阻塞、死锁等很有用。

+ jconsole 或 jstat 可以提供应用已使用类的个数。 jstat 还能提供类编译相关的信息。

+ 几乎所有的监控工具都能报告一些 GC 活动的信息。 jconsole 可以用实时图显示堆的使用情况。 jcmd 可以执行 GC 操作。 jmap 可以打印堆的概况、永久代信息或者创建堆转储。jstat 可以为垃圾收集器正在执行的操作生成许多视图。

+ jvisualvm 的 GUI 界面可以捕获堆转储，也可以用命令行 jcmd 或 jmap 生成。堆转储是堆使用情况的快照，可以用不同的工具进行分析，包括 jvisualvm 和 jhat，或者外部工具，如：Eclipse Memory Analyzer Tool等。

+ 几乎所有的 Java 性能分析工具都是用 Java 写的，并以“关联” （attaching）应用的方式进行性能分析——意思是性能分析器开启与目标应用之间的 socket（或其他通信通道） 。随后目标应用和性能分析工具交换应用的行为信息。

+ 性能分析有两种模式：数据采样或数据探查。数据采样是性能分析的基本模式，带来的开销最小，这点很重要。性能分析为人诟病的一点就是，对应用进行的测量会改变它的性能。 

+ 探查分析器相比于采样分析器，侵入性更强，但它们可以给出关于程序内部所发生的更有价值的信息。探查分析器会在类加载时更改类的字节码（即插入统计调用次数的代码
等） 。相比采样分析器，探查分析器更可能会将性能偏差引入应用。

+ 优先优化哪些调用次数更多的方法，会获得更大的性能提升。

+ 如果希望能看到那些阻塞调用所花费的时间，即线程在 wait() ——等待其他线程唤醒——中的用时决定了许多应用的整体执行时间。那么在大多数基于 Java 的性能分析器可以通过设置过滤器和调整其他选项来显示或隐藏这些阻塞方法。

+ Oracle Solaris Studio是一个非常有用的分析Linux的性能分析器。

+ 基于Java 的性能分析工具所不能提供的——应用花在 GC 上的时间。在基于 Java 的性能分析工具中，GC 线程的影响几乎看不到。 （除非测试所运行的机器是 CPU 受限，否则编译器线程占用大量时间并无大碍：虽然编译线程会消耗大量的 CPU 时间，但只要机器上有更多可用的 CPU，应用自身就不受影响，因为编译是在后台发生。 ）因此，可以考虑使用Oracle Solaris Studio等c/c++性能分析器。

+ 性能分析器是查找性能瓶颈最重要的工具，但你必须学会如何使用它们，然后找到需要优化的代码区域。

+ 商业版 Java 7（从 7u40 开始）和 Java 8 包含了称为 Java Mission Control（以下称 JMC）的监控新特性。该特性不开源，需要付费购买。

+ JMC 的关键特性是 Java 飞行记录器（Java Flight Recorder，JFR） 。正像它名字所暗示的，JFR 数据是 JVM 的历史事件，这些可以用来诊断 JVM 的历史性能和操作。JFR 的基本操作是开启一组事件（例如，线程等待某个锁而被阻塞的事件） 。每当选择的事件发生时，就会保存相应的数据（保存在内存或文件中） 。数据流保存在循环缓冲中，所以只有最近的事件。JMC 可以显示这些事件——实时从 JVM 获取或者从文件读取——你可以对这些事件进行分析，诊断性能问题。 JFR的详细使用，见书中。

+ 