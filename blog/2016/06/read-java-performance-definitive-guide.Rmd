---
author: "patrick.unicorn"
output:
  html_document:
    css: ../../css/main.css
    highlight: null
    theme: null
---

读《Java Performance: The Definitive Guide》

* * *

## 基本概念

+ JVM 性能调优的过程实际上与 C++ 程序员在编译时通过测试选择编译参数，以及 PHP 码农在 php.ini
文件中选择适当变量等过程非常类似。

+ 编码和调优常常被认为是两个不同的专业领域：性能调优工程师只是竭力将 JVM的性能发挥到极致，而开发人员只关心他们的代码逻辑是否正确。这种区分没有什么意义。任何从事Java相关工作的人都应该熟谙代码在 JVM 中的行为，以及如何调优才能提升性能。

+ 商业版 JVM 中和性能密切相关的一个特性就是 Java 飞行记录器（Java Flight Recorder，JFR）。

+ 除了少数例外，JVM 主要接受两类标志：布尔标志和附带参数的标志。

+ 布尔标志采用以下语法： -XX:+FlagName 表示开启， -XX:-FlagName 表示关闭。

+ 附带参数的标志采用以下语法： -XX:FlagName=something ，表示将标志 flagName 的值设置为 something 。其中 something 通常可以为任意值。例如 -XX:NewRatio=N ，表示 NewRatio可以设置为任意值 N。

+ 在给定的命令行上，添加 -XX:+Printflagsfinal （默认为 false ，即“关闭” ）就能获得具体运行环境中特定标志的默认值。

+ 需要更高性能时，算法是否优秀就是重中之重了。

+ 要创建和销毁的对象越多，垃圾收集的工作量就越大；要分配和持有的对象越多，GC 的周期就越长；要从磁盘装载进 JVM 的类越
多，程序启动所花费的时间就越长；要执行的代码越多，机器硬件缓存的效率就越低；而执行的代码越多，花费的时间就越长。

+ 随着新特性的添加和新要求的采纳（为了与对手竞争） ，程序会越来越大，越来越慢。

+ “我们不应该把大量时间都耗费在那些小的性能改进上；过早考虑优化是所有噩梦的根源” 。这句名言的重点是，最终你应该编写清晰、直接、易读和易理解的代码。这里的“优化”应该理解为虽然算法和设计改变了复杂程序的结构，但是提供了更好的性能。那些真正的优化最好留到以后，等到性能分析表明这些措施有巨大收益的时候才进行。而这里所指的过早优化，并不包括避免那些已经知道对性能不好的代码结构。每行代码，如果有两种简单、直接的编程方式，那就应该选择性能更好的那种。

+ 常用的打印日志中，如果有拼接字符串的消息，应该首先判断日志是否开启，以避免不必要的字符串连接开销，比如：

```
// 不好
log.log(Level.FINE, "I am here, and the value of X is" 
    + calcX() + " and Y is " + calcY());
    
// 优化
if (log.isLoggable(Level.FINE)) {
    log.log(Level.FINE,
        "I am here, and the value of X is {} and Y is {}",
        new Object[]{calcX(), calcY()});
}
```

+ 在分布式环境中，比如 Java EE 应用服务器、负载均衡器、数据库和后台企业信息系统，Java 应用服务
器的性能问题可能只是其中很小的部分。

+ 对于整体系统，我们需要采取结构化方法针对系统的所有方面分析性能。CPU使用率、I/O延迟、系统整体的吞吐量都必须测量和分析。只有到那时，我们才能判定到底是哪个组件导致了性能瓶颈。

+ 增加系统某个组件的负载从而导致整个系统性能变慢，这项原则不仅限于数据库。CPU密集型的应用服务器增加负载，或者越来越多线程试图获取已经有线程等待的锁，还有许多其他场景，也都适用这项原则。

+ 优化的原则：

<blockquote>
+ 借助性能分析来优化代码，重点关注性能分析中最耗时的操作。然而请注意，这并不意
味着只看性能分析中的叶子方法 。

+ 利用奥卡姆剃刀原则诊断性能问题。性能问题最可能的原因应该是最容易解释的：新代
码比机器配置更可能引入性能问题，而机器配置比 JVM 或者操作系统的 bug 更容易引
入性能问题。隐藏的 bug 确实存在，但不应该把最可能引起性能问题的原因首先归咎于
它，而只在测试用例通过某种方式触发了隐藏的 bug 时才关注。但不应该一上来就跳到
这种不太可能的场景。

+ 为应用中最常用的操作编写简单算法。以估算数学公式的程序为例，用户可以决定他所
期望的最大容许误差为 10% 或 1%。如果 10% 的误差适合多数用户，那么优化代码就
意味着即便误差范围缩小为 1%，但是速度变慢了
</blockquote>

+ **微基准测试**用来测量微小代码单元的性能，包括调用同步方法的用时与非同步方法的用时比较，创建线程的代价与使用线程池的代价，执行某种算法的耗时与其替代实现的耗时，等等。相当于测试函数或代码片段。

+ 编写多线程微基准测试时务必深思熟虑。当若干个线程同时执行小段代码时，极有可能会产生同步瓶颈（以及其他线程问题） 。所以，如果我们过多依赖多线程基准测试的结果，就常常会将大量时间花费在优化那些真实场景中很少出现的同步瓶颈上，而不是性能需求更迫切的地方。

+ 考虑这样的微基准测试，即有两个线程同时调用同步方法。由于基准测试的代码量相对于被测方法来说比较少，所以多数时间都是在执行同步方法。假设执行同步方法的时间只占整个微基准测试的 50%，即便少到只有两个线程，同时执行同步代码的概率仍然很高。因此基准测试运行得很慢，并且随着线程数的增加，竞争所导致的性能问题将愈演愈烈。最终结果就是，测试衡量的是 JVM 如何处理竞争，而不是微基准测试的本来目的。

+ 微基准测试中的输入值必须事先计算好，不能使用随机数生成函数放在测试代码中，因为，随机生成函数也要耗费时间。

+ 我们应该为常见的场景进行优化，即使需要加入特殊的处理逻辑。

+ Java 的一个特点就是代码执行的越多性能越好，基于这点，微基准测试应该包括热身期，使得编译器能生成优化的代码。

+ 在做回归测试的时候，追踪级别设为纳秒很有意义。如果集合操作每次都节约几纳秒，日积月累下来意义就很重大了。对于那些不频繁的操作来说，例如那种同时只需处理一个请求的 servlet，修复微基
准测试所发现的纳秒级性能衰减就是浪费时间，这些时间用在优化其他操作上可能会更有价值。微基准测试难于编写，真正管用的又很有限。所以，应该了解这些相关的隐患后再做出决定，是微基准测试合情合理值得做，还是关注宏观的测试更好。

+ 衡量应用性能最好的事物就是应用自身，以及它所用到的外部资源。如果正常情况下应用需要调用 LDAP 来检验用户凭证，那应用就应该在这种模式下测试。虽然删空 LDAP 调用在模块测试中有一定意义，但应用本身必须在完整真实配置的环境中测试。

+ 任何应用（包含独立运行的 JVM）都可以像这样划分成一系列步骤，方框中的模块、子系统等产生数据的速度取决于它们的效率。（耗费的时间包括子系统代码的执行时间，也包括网络传输的时间、磁盘传输的时间，等等。如果是模块化的模型，时间应该只包括该模块内代码的执行时间。）数据进入子系统的速率取决于前一个模块或系统的输出速率。

+ 全应用测试有个很重要的场景，就是同一台机器上同时运行多个应用。许多 JVM 的默认调优都是默认假定整个机器的资源都归 JVM 使用。如果单独测试，优化效果很好。如果在其他应用（包括但不限于 Java 程序）运行的时候进行测试，性能会有很大的不同。

+ 单个 JVM（默认配置）执行 GC 周期时，该机器上所有处理器的 CPU 使用率都会变成 100%。如果测量程序执行时的平均CPU 使用率，大概会有 40%——实际意思是，某些时候 30%的CPU被占用，其他时候为100%。当隔离 JVM 时，它可以运行得很好，但如果 JVM 与其他应用并发运行，它就不可能在 GC 时获得 100% 的CPU。此时测出来的性能会与它单独运行时不同。这是微基准测试和模块测试不可能让你全面了解应用性能的另一个原因。

+ 不进行整体应用的测试，就不可能知道哪部分的优化会产生回报。

+ 使 Java 代码更快的常用方法是更好的算法，这不依赖Java 调优或者 Java 编码实践。

+ 虚拟机会花几分钟（或更长时间）全面优化代码并以最高性能执行。由于这个（以及其他）原因，研究Java的性能优化就要密切注意代码优化的热身期：大多数时候，应该在运行代码执行足够长时间，已经编译并优化之后再测量性能。

+ 许多情况下应用从开始到结束的整体性能更为重要。报告生成器处理 10 000 个数据元素需要花费大量时间，但对最终用户而言，处理前 5000个元素是否比后5000个慢50%并不重要。即便是像应用服务器这样的系统——其性能必定会随运行时间而改善——初始的性能依然很重要。某种配置下的应用服务器需要 45 分钟才能达到性能峰值。

+ 在客户端 - 服务器的吞吐量测试中，并不考虑客户端的思考时间。客户端向服务器发送请求，当它收到响应时，立刻发送新的请求。持续这样的过程，等到测试结束时，客户端会报告它所完成的操作总量。客户端常常有多个线处理，所以吞吐量就是所有客户端所完成的操作总量。通常这个数字就是每秒完成的操作量，而不是测量期间的总操作量。这个指标常常被称作每秒事务数（TPS） 、每秒请求数（RPS）或每秒操作次数（OPS） 。

+ 响应时间测试和吞吐量测试（假设后者是基于客户端 - 服务器模式）之间的差别是，响应时间测试中的客户端线程会在操作之间休眠一段时间。这被称为思考时间。响应时间测试是尽量模拟用户行为：用户在浏览器输入URL，用一些时间阅读返回的网页，然后点击页面上的链接，花一些时间阅读返回的网页，等等。当测试中引入思考时间时，吞吐量就固定了：指定数量的客户端，在给定思考时间下总是得到相同的 TPS（少许差别，参见框注） 。基于这点，测量请求的响应时间就变得重要了：服务器的效率取决于它响应固定负载有多快。

+ 性能测试的结果会随时间而变。即便程序每次处理的数据集都相同，产生的结果也仍然会有差别。因为有很多因素会影响程序的运行，如机器上的后台进程，网络时不时的拥堵等。好的基准测试不会每次都处理相同的数据集，而是会在测试中制造一些随机行为以模拟真实的世界。这就会带来一个问题：运行结果之间的差别，到底是因为性能有变化，还是因为测试的随机性。可用以下方法来解决这个问题，即多次运行测试，然后对结果求平均。当被测代码发生变化时，就再多次运行测试，对结果求平均，然后比较两个平均值。

+ 比较基准测试的结果时，我们不可能知道平均值的差异是真的性能有差还是随机涨落。最好的办法是先假设“平均值是一样的” ，然后确定该命题为真时的几率。如果命题很大几率为假，我们就有信心认为平均值是真的有差别（虽然我们永远无法 100% 肯定） 。注：《测试技术计量》课程，有一些对实验结果进行比对的方法。

+ 因代码更改而进行的测试称为回归测试。在回归测试中，原先的代码称为基线（baseline） ，而新的代码称为试样（specimen） 。

+ 所有的客户端 - 服务器测试都存在风险，即客户端不能足够快地向服务器发送数据。这可能是由于客户端机器的 CPU 不足以支持所需数量的客户端线程，也可能是因为客户端需要花大量时间处理响应才能发送新的请求。在这些场景中，测试衡量的其实是客户端性能而不是服务器性能的。其中的风险依赖于每个线程所承载的工作量（客户端机器的线程数和配置） 。由于客户端线程需要执行大量工作，零思考时间（面向吞吐量）测试更可能会遇到这种情形。因此，通常吞吐量测试比响应时间测试的线程数少，线程负载也小。通常吞吐量测试也会报告请求的平均响应时间。但平均响应时间的变化并不表示性能有问题，除非报告的吞吐量相同。能够承受 500 OPS、响应时间 0.5 秒的服务器，它的性能要好过响应时间 0.3 秒但只有 400 OPS 的服务器。吞吐量测试总是在合适的热身期之后进行。

+ 衡量响应时间有两种方法。响应时间可以报告为平均值：请求时间的总和除以请求数。响应时间也可以报告为百分位请求，例如第 90 百分位响应时间。如果 90% 的请求响应小于1.5 秒，且 10% 的请求响应不小于 1.5 秒，则 1.5 秒就是第 90 百分位响应时间。两种方法的一个区别在于，平均值会受离群值影响。这是因为计算平均值时包括了离群值。离群值越大，对平均响应时间的影响就会越大。对于Java应用来说，由于GC的存在，更容易引起这样的离群值。因此，一般最好同时考虑**平均响应时间**和至少一种**百分位响应时间**。

+  Faban（http://faban.org/）为例，这是一个开源的、基于 Java 的负载生成器。

+ 查找代码性能变化的正确方法是先决定一个显著性水平——比如 0.1——然后用 t 检验判定在这个显著性水平上试样是否与基线有差别。

```
统计学中的显著性与重要性

显著性差异并不意味着统计结果对我们更重要。平均为 1 秒的变化很小的基线，和平均为 1.01 秒的变化很小的试样，其 p 值可能为 0.01：结果的差别有 99% 的置信度。
但结果的差别只有 1%。现在假定另外一个测试，试样和基线有 10% 的变动，但是 p值为 0.2，即非统计显著。哪个测试的结果最为重要？这需要更多时间来审查。审查后发现虽然相差 10% 的测试的置信度低，但在用时上更加优化（如果可能的话，可以用更多数据来验证测试结果是否真的统计显著） 。仅仅因为 1% 差异的可能性更大，并不意味着它更重要。
```

+  Apache Commons Mathematics 类库中的 TTest 可以计算统计结果的**置信度**。

+ 程序时不时会表现出随机行为，而一旦引入了随机性，我们就再也无法 100% 确定这些数据意味着什么了。使用统计分析有助于使结果变得更客观，但即便如此，仍然免不了主观臆断。理解这些数据背后的概率及其意义，有助于降低主观性。

## 性能调优工具

+ 通常 CPU 使用率可以分为两类：用户态时间和系统态时间（Windows 上被称作 privileged time） 。用户态时间是 CPU 执行应用代码所占时间的百分比，而系统态时间则是 CPU 执行内核代码所占时间的百分比。系统态时间与应用相关，比如应用执行 I/O 操作，系统就会执行内核代码从磁盘读取文件，或者将缓冲数据发送到网络，等等。任何使用底层系统资源的操作，都会导致应用占用更多的系统态时间。性能调优的目的是，在尽可能短的时间内让 CPU 使用率尽可能地高。

+ CPU 使用率是一段时间内的平均数——5 秒、30 秒，也可能只有 1秒那么短（不过永远不会比这还要短） 。比如，10 分钟内一个程序执行的 CPU 使用率为50%。如果代码调优之后，CPU 使用率达到了 100%，说明程序的性能翻了倍：程序只需要执行 5 分钟就可以了。如果性能再翻倍，CPU 仍将是 100%，而执行完程序只要 2.5 分钟。CPU 使用率表示程序以多高的效率使用 CPU，所以数字越大，性能越好。

+ Linux查看CPU情况的工具是：**vmstat**。

+ 提高 CPU 使用率，一直都是批处理任务的目的。如果 CPU已经达到 100%，你仍然可以寻找优化，使得工作完成的更快（也要尽量保持 100%CPU 使
用率） 。

+ 一般来说，多 CPU 多线程的目的仍然是通过不阻塞线程来提高 CPU 使用率，或者是在线程完成工作等待更多任务时降低 CPU 使用率。

+ 在多线程多 CPU 下，需要重点考虑以下 CPU 空闲的情形：即便有事可做，CPU 仍然空闲。这在程序没有更多线程可用的时候可能会出现。典型的情况是，应用以固定尺寸
的线程池运行各种任务。每个线程同时只能执行一个任务，当线程被某个任务阻塞时（例如，等待数据库的响应） ，它就没法捡出新任务执行了。所以此时的情况就是，有任务需要执行（有事可做） ，却没有线程执行它们，结果就是 CPU 处在空闲时间。

+ Windows 和 Unix 系统都可以监控可运行（意味着没有被 I/O 阻塞、休眠等）的线程数。Unix 系统称之为运行队列（run queue） ，vmstat输出中就有。Windows 将这个数字称为处理器队列（processor quque），使用typeperf查看。但是需要注意：Unix 系统的运行队列长度（ vmstat 输出）是所有正在运行或待运行（即一旦有可用 CPU就可以运行）的线程数。而在 Windows 中，处理器队列长度就不包括正在运行的线程。

+ 如果试图运行的线程数超过了可用的 CPU，性能就会下降。一般来说，Windows 的处理器队列长度最好为 0，小于或等于 Unix 系统 CPU 的数目。如果在相当长时间内运行队列很长，说明系统已经过载，这时你应该检查系统，减少机器正在处理的工作量（将工作转移到其他机器或者优化代码） 。

+ 检查应用性能时，首先应该审查 CPU 时间。优化代码的目的是提升而不是降低（更短时间段内的）CPU 使用率。在试图深入优化应用前，应该先弄清楚为何 CPU 使用率低。

+ 监控磁盘使用率有两个目的。第一个目的与应用本身有关：如果应用正在做大量的磁盘I/O 操作，那 I/O 就很容易成为瓶颈。想了解何时磁盘 I/O 是瓶颈非常困难，因为这取决于应用的行为。

+ Linux系统使用**iostat**查看磁盘情况。

+ Linux 中可以查看进程的 47.89%的事件在 iowait （表示正在等待磁盘） ，这时表示磁盘速度赶不上IO请求了。

+ 监控磁盘使用率的第二个理由是——即便预计应用不会有很高的 I/O——有助于监控系统是否在进行内存交换。正在内存交换的系统——从主内存移动数据到磁盘或者反过来——一般来说，性能比较差。还有其他系统工具可以报告系统交换，例如 vmstat 输出中有两列（ si 是换进， so 是换出）可以警告我们系统是否正在交换。

+ 写入磁盘的应用遇到瓶颈，是因为写入数据的效率不高（吞吐量太低） ，或者是因为写入太多数据（吞吐量太高） 。

+ 网络使用率类似磁盘流量：应用可能没有充分利用网络所以带宽很低，或者写入某网络接口的总数据量超过了它所能处理的量。

+ 由于标准的系统工具通常只能显示某个网络接口发送和接收的数据报数和字节数，所以它们在监控网络流量方面差强人意。虽然这些信息有用，但无法告诉我们网络是没有充分利用，还是过度使用。

+ Unix 系统监控网络的基本工具是 netstat （大多数 Linux 发行版中还没有包括 netstat ，必须单独获得） 。Windows 上则可以在脚本中使用 typeperf ，监控网络使用率.

+ 有许多开源和商业工具可以监控网络带宽。Unix 里一个受欢迎的命令行工具就是 nicstat ，它可以显示每个网络接口的流量概要，包括网络接口的使用度。

+ typeperf 或 netstat 这样的工具可以报告读取和写入的数据，但是要计算网络使用率，你必须自己用脚本计算接口的带宽。虽然一般工具报告的单位是字节 / 秒（Bps） ，但请切记，带宽的单位是位 / 秒（bps） 。1000 兆位网络每秒处理 125 兆字节（MB） 。

+ 网络无法支持 100% 的使用率。对本地以太局域网来说，承受的网络使用率超过 40% 就意味着接口饱和了。如果网络是包交换或使用不同的传输介质，网络使用率的最大值就可能会不同，因此最好是评估网络架构之后再确定合适的值。这个值与 Java 无关，只是简单利用网络参数和操作系统接口。

+ 对基于网络的应用来说，务必要监控网络以确保它不是瓶颈。

+ `jcmd <process id> <command>` 用来打印Java进程的所涉及的基本类、线程和VM信息。

+ jconsole提供JVM活动的GUI视图。

+ jhat用于读取内存堆dump文件，用于分析。

+ jmap用于内存堆或其他JVM内存信息的dump。

+ jinfo查看JVM的系统属性，可以动态设置一些系统属性。

+ jstack转储Java进程中的线程栈信息，可以用于分析死锁的等。

+ jstat提供GC和类装载的信息。

+ jvm的GUI工具，可以用来剖析应用中的应用，也可以用来分析内存堆dump文件。

+ `jcmd process_id VM.uptime`或`jinfo -sysprops process_id`查看JVM运行多长时间了。

+ jconsole 的“VM 摘要”页可以显示程序所用的命令行，或者用 jcmd 显示`jcmd process_id VM.command_line`。

+ `jcmd process_id VM.flags [-all]`可以查看JVM的所有设置属性信息。类似与Gradle的`gradle properties`。

+ 诊断性能问题时，找出哪些标志起作用是很常见的事。JVM 运行时，可以用 jcmd 做到这一点。如果想找出特定 JVM 的平台特定的默认值是什么，那么在命令行上添加 -XX:+Printflagsfinal 会很有用：`java other_options -XX:+PrintFlagsFinal -version`，其与上面一项中的jcmd命令输出结果一样。

```
输出结果：uintx InitialHeapSize := 4169431040 {product / pd product / manageable /  C2 diagnostic}

1. 冒号表示标志使用的是非认值
2. product 表示在所有平台上的默认设置都是一致的。 
3. pd product 表示标志的默认值是独立于平台的。
4. manageable 表示运行时可以动态更改标志的值。
5. C2 diagnostic表示为编译器工程师提供诊断输出，帮助理解编译器正以什么方式运作
```

+ 另一种查看运行中的应用的此类信息的工具，叫作 jinfo 。 jinfo 的好处在于，它允许程序在执行时更改某个标志的值，并且可以单独检查输出单个标志的值。

```
jinfo 可以更改任意标志的值，但并不意味着 JVM 会响应更改。比如说，大多数影响 GC 算法行为的标志都在启动时使用，以决定垃圾收集器的行为方式。之后通过 jinfo 更改标志值，并不会导致 JVM 改变它的行为。它会以初始时的算法继续执行。所以这个技术只会对那些在 Printflagsfinal 输出中标记为 manageable 的标志有效
```

+ `jstack process_id`或者`jcmd process_id Thread.print`可以打印出每个线程的详细输出，对于分析线程阻塞、死锁等很有用。

+ jconsole 或 jstat 可以提供应用已使用类的个数。 jstat 还能提供类编译相关的信息。

+ 几乎所有的监控工具都能报告一些 GC 活动的信息。 jconsole 可以用实时图显示堆的使用情况。 jcmd 可以执行 GC 操作。 jmap 可以打印堆的概况、永久代信息或者创建堆转储。jstat 可以为垃圾收集器正在执行的操作生成许多视图。

+ jvisualvm 的 GUI 界面可以捕获堆转储，也可以用命令行 jcmd 或 jmap 生成。堆转储是堆使用情况的快照，可以用不同的工具进行分析，包括 jvisualvm 和 jhat，或者外部工具，如：Eclipse Memory Analyzer Tool等。

+ 几乎所有的 Java 性能分析工具都是用 Java 写的，并以“关联” （attaching）应用的方式进行性能分析——意思是性能分析器开启与目标应用之间的 socket（或其他通信通道） 。随后目标应用和性能分析工具交换应用的行为信息。

+ 性能分析有两种模式：数据采样或数据探查。数据采样是性能分析的基本模式，带来的开销最小，这点很重要。性能分析为人诟病的一点就是，对应用进行的测量会改变它的性能。 

+ 探查分析器相比于采样分析器，侵入性更强，但它们可以给出关于程序内部所发生的更有价值的信息。探查分析器会在类加载时更改类的字节码（即插入统计调用次数的代码
等） 。相比采样分析器，探查分析器更可能会将性能偏差引入应用。

+ 优先优化哪些调用次数更多的方法，会获得更大的性能提升。

+ 如果希望能看到那些阻塞调用所花费的时间，即线程在 wait() ——等待其他线程唤醒——中的用时决定了许多应用的整体执行时间。那么在大多数基于 Java 的性能分析器可以通过设置过滤器和调整其他选项来显示或隐藏这些阻塞方法。

+ Oracle Solaris Studio是一个非常有用的分析Linux的性能分析器。

+ 基于Java 的性能分析工具所不能提供的——应用花在 GC 上的时间。在基于 Java 的性能分析工具中，GC 线程的影响几乎看不到。 （除非测试所运行的机器是 CPU 受限，否则编译器线程占用大量时间并无大碍：虽然编译线程会消耗大量的 CPU 时间，但只要机器上有更多可用的 CPU，应用自身就不受影响，因为编译是在后台发生。 ）因此，可以考虑使用Oracle Solaris Studio等c/c++性能分析器。

+ 性能分析器是查找性能瓶颈最重要的工具，但你必须学会如何使用它们，然后找到需要优化的代码区域。

+ 商业版 Java 7（从 7u40 开始）和 Java 8 包含了称为 Java Mission Control（以下称 JMC）的监控新特性。该特性不开源，需要付费购买。

+ JMC 的关键特性是 Java 飞行记录器（Java Flight Recorder，JFR） 。正像它名字所暗示的，JFR 数据是 JVM 的历史事件，这些可以用来诊断 JVM 的历史性能和操作。JFR 的基本操作是开启一组事件（例如，线程等待某个锁而被阻塞的事件） 。每当选择的事件发生时，就会保存相应的数据（保存在内存或文件中） 。数据流保存在循环缓冲中，所以只有最近的事件。JMC 可以显示这些事件——实时从 JVM 获取或者从文件读取——你可以对这些事件进行分析，诊断性能问题。 JFR的详细使用，见书中。

## Java编译器优化

+ 大多数情况下,所谓编译器调优,其实就只是为目标机器上的 Java 选择正确的 JVM 和编 译器开关(-client、-server 或 -XX:+TieredCompilation)而已。分层编译通常是长期运 行应用的最佳选择,而对于运行时间短的应用来说,分层编译与 client 编译器的性能差别很小。

+ 确实没有什么好的机制可以算出程序所需要的代码缓存。所以,如何增加代码缓存,基本 上就是摸着石头过河,通常的做法是简单地增加 1 倍或 3 倍。-XX:ReservedCodeCacheSize=N(对特定编译器来说,N 为默认的值)标志可以设置代码缓存的最大值。代码缓存的管理和大多数 JVM 内存一样,有初始值(由 -XX:InitialCodeCacheSize=N指定)。代码缓存从初始大小开始分配,一旦充满就会增加(直至最大值)。

+ 理解 JVM 保留内存和分配内存方式之间的差别非常重要。这种差别在代码缓存、Java 堆以及其他 JVM 本地内存结构中都存在。

+ 代码缓存设为 1 GB,JVM 就会保留 1 GB 的本地内存空间。虽然这部分内存在需要时才会分配,但它仍然是被保留的。

+ 分层编译很容易达到代码缓存默认配置的上限(特别是在 Java 7 中)。使 用分层编译时,应该监控代码缓存,必要时应该增加它的大小。

+ 触发代码编译的条件。其中最主要的因素是代码执行的频度。一旦执行达到一定次数,且达到了编译阈值,编译器就可以获得足够的信息编译代码了。

+ 编译是基于两种 JVM 计数器的:方法调用计数器和方法中的循环回边计数器。回边实际上可看作是循环完成执行的次数,所谓循环完成执行,包括达到循环自身的末尾,也包括 执行了像 continue 这样的分支语句。

+ JVM 执行某个 Java 方法时,会检查该方法的两种计数器总数,然后判定该方法是否适合编译。如果适合,该方法就进入编译队列。这种编译没有正式的名称,通常叫标准编译。

+ 如果循环真的很长——或因包含所有程序逻辑而永远不退出,又该如何?在这种情 况下,JVM不等方法被调用就会编译循环。所以循环每完成一轮,回边计数器就会增加并被检测。如果循环的回边计数器超过阈值,那这个循环(不是整个方法)就可以被编译。由于仅仅编译循环还不够,JVM必须在循环进行的时候还能编译循环。在循环代码编译结束后,JVM 就会替换还在栈上的代码,循环的下一次迭代就会执行快得多的编译代码。故而称为栈上替换(On-Stack Replacement,OSR)。

+ 标准编译由 -XX:CompileThreshold=N 标志触发。使用 client 编译器时,N 的默认值是 1500, 使用 server 编译器时为 10 000。更改 CompileThreshold 标志的值,将使编译器提早(或延 后)编译。然而请注意,尽管有一个标志,但这个标志的阈值等于回边计数器加上方法调 用计数器的总和。

+ 更改 OSR 编译阈值的情况非常罕见。事实上,虽然 OSR 编译在基准测试(特别是微基准测试)中经常发生,但在实际运行时并不经常出现。

+ VM 供应商提交的基准测试已经验证过上述调优,不同设置的基准测试间并没有什么性能差异。他们使用较低的设置主要基于以下两个原因:
    1. 节约一点应用热身的时间;
    2. 使得某些原本可能不会被 server 编译器编译的方法得以编译。
    
<blockquote>
每种计数器的值都会周期性减少(特别是当 JVM 达到安全点时)。

实际上,计数器只是方法或循环最新热度的度量。由此带来的一个副作用是,
执行不太频繁的代码可能永远不会编译,即便是永远运行的程序(相对于热来说,有时称这些方法为温热 [lukewarm])。
这就是通过减少编译阈值来进行优化的一种情况,它也是分层编译通常比单独的 server 编译器 要快的原因之一。
</blockquote>

+ 如果开启 PrintCompilation,每次编译一个方法(或循环)时,JVM 就会打印一行被编译 的内容信息。输出的信息在不同的 Java 发布版之间会有所不同，绝大多数编译日志的行具有以下格式: 

```
timestamp compilation_id attributes (tiered_level) method_name size deopt 

此处的时间戳 timestamp 是编译完成的时间(相对于 JVM 开始的时间 0)。

compilation_id 是内部的任务 ID。通常这个数字只是简单地单调增长,不过在使用 server编译器时(或者某个时刻编译器的线程数增加时),你有时会发现乱序的compilation_id。


attributes 是一组 5 个字符长的串,表示代码编译的状态。
5字符属性串可以同时出现 2 个或多个字符。不同的属性如下所列: 

• %:编译为 OSR。
• s:方法是同步的。
• !:方法有异常处理器。
• b:阻塞模式时发生的编译。
• n:为封装本地方法所发生的编译。

n 属性表明 JVM 生成了一些编 译代码以便于调用本地方法。
size是编译后代码的大小(单位是字节)。这是 Java 字节码的大小,不是被编译代码的大小.
```

+ jstat 有两个有关编译器信息的标志。jstat -compiler 标志提供了关于多少方法被编译的概
要信息;可以用 jstat -printcompilation 标志获取最近被编译的方法。jstat 借助一个可选 参数反复执行操作,你可以看到随时间变化有哪些方法被编译了。

+ 书中的《高级编译器调优》一节主要是针对JVM工程师调优JVM的，一般开发者不必须理解。

+ PrintCompilation 标志输出的讨论中曾提到两种代码逆优化的情况。逆优化意味着编译器 不得不“撤销”之前的某些编译;结果是应用的性能降低，有两种逆优化的情形:代码状态分别为“made not entrant”(代码被丢弃)和“made zombie”(产生僵尸代码)时。

+ 大概来说，在分层编译中,代码先由 client 编译器编译,然 后由 server 编译器编译。

+ 一般来说,像僵尸代码重编译 这样小的操作对大多数应用都不会有显著的影响。

### Java垃圾收集

+ 垃圾收集由两步构成:查找不再使用的对象,以及释放这些对象所管理的内存。

+ 垃圾收集的性能就是由这些基本操作所决定的: 找到不再使用的对象、回收它们使用的内存、对堆的内存布局进行压缩整理。

+ 垃圾收集器回收对象,或者在内存中移动对象时,必须确 保应用程序线程不再继续使用这些对象。这一点在收集器移动对象时尤其重要:在操作过 程中,对象的内存地址会发生变化,因此这个过程中任何应用线程都不应再访问该对象。所有应用线程都停止运行所产生的停顿被称为时空停顿(stop-the-world)。通常这些停顿对 应用的性能影响最大,调优垃圾收集时,尽量减少这种停顿是最为关键的考量因素。

+ 所有的垃圾收集器都遵循了同一个方式,即根据情况将 堆划分成不同的代(Generation)。这些代被称为“老年代”(Old Generation 或 Tenured Generation)和“新生代”(Young Generation)。新生代又被进一步地划分为不同的区段, 分别称为 Eden 空间和 Survivor 空间(不过 Eden 有时会被错误地用于指代整个新生代)。

+ 采用分代机制的原因是很多对象的生存时间非常短。

+ 对象首先在新生 代中分配。新生代填满时,垃圾收集器会暂停所有的应用线程,回收新生代空间。不再使用 的对象会被回收,仍然在使用的对象会被移动到其他地方。这种操作被称为 Minor GC。采用这种设计有两个性能上的优势。其一,由于新生代仅是堆的一部分,与处理整个堆相 比,处理新生代的速度更快。第二个优势源于新生代中对象分配的方式。对象分配于 Eden 空间(占据了新生代空间的 绝大多数)。垃圾收集时,新生代空间被清空,Eden 空间中的对象要么被移走,要么被回 收;所有的存活对象要么被移动到另一个 Survivor 空间,要么被移动到老年代。由于所有 的对象都被移走,相当于新生代空间在垃圾收集时自动地进行了一次压缩整理。

+ 所有的垃圾收集算法在对新生代进行垃圾回收时都存在“时空停顿”现象。

+ 对象不断地被移动到老年代,最终老年代也会被填满,JVM 需要找出老年代中不再使用 的对象,并对它们进行回收。而这便是垃圾收集算法差异最大的地方。简单的垃圾收集算法直接停掉所有的应用线程,找出不再使用的对象,对其进行回收,接着对堆空间进行整 理。这个过程被称为 Full GC,通常导致应用程序线程长时间的停顿。

+ 有可能在应用线程运行的同时找出不再使用的对象;CMS 和 G1 收集器就是通过这种方式进行垃圾收集的。由于它们不需要停止应用线程 就能找出不再用的对象,CMS 和 G1 收集器被称为 Concurrent 垃圾收集器。同时,由于它 们将停止应用程序的可能降到了最小,也被称为低停顿(Low-Pause)收集器(有时也称 为无停顿收集器,虽然这个叫法相当不确切)。Concurrent收集器也使用各种不同的方法对老年代空间进行压缩。
使用 CMS 或 G1 收集器时,应用程序经历的停顿会更少(也更短)。其代价是应用程序会消耗更多的CPU。CMS 和 G1 收集也可能遭遇长时间的 Full GC 停顿(尽量避免发生那样 的停顿是这些调优算法要考虑的重要方面)。

+ 评估垃圾收集器时,想想你需要达到的整体性能目标。每一个决定都需要权衡取舍。如果 应用对单个请求的响应时间有要求(譬如 Java 企业版服务器),你应该考虑下面这些因素。
• 单个请求会受停顿时间的影响——不过其受 Full GC 长时间停顿的影响更大。如果目标 是要尽可能地缩短响应时间,那么选择使用 Concurrent 收集器更合适。
• 如果平均响应时间比最大响应时间更重要(譬如 90% 的响应时间),采用 Throughput 收 集器通常就能满足要求。
• 使用 Concurrent 收集器来避免长的停顿时间也有其代价,这会消耗额外的 CPU。

+ 为批量应用选择垃圾收集器可以遵循下面的原则。
• 如果 CPU 足够强劲,使用 Concurrent 收集器避免发生 Full GC 停顿可以让任务运行得 更快。
• 如果 CPU 有限,那么 Concurrent 收集器额外的 CPU 消耗会让批量任务消耗更多的时间。

+ JVM 提供了以下 4 种不同的垃圾收集算法：Serial垃圾收集器，Throughput垃圾收集器，CMS收集器，G1垃圾收集器。

+ Serial 垃圾收集器是四种垃圾收集器中最简单的一种。如果应用运行在 Client 型虚拟机 (Windows 平台上的 32 位 JVM 或者是运行在单处理器机器上的 JVM)上,这也是默认的
垃圾收集器。
Serial 收集器使用单线程清理堆的内容。使用 Serial 收集器,无论是进行 Minor GC 还是 Full GC,清理堆空间时,所有的应用线程都会被暂停。进行 Full GC 时,它还会对老年代 空间的对象进行压缩整理。通过 -XX:+UseSerialGC 标志可以启用 Serial 收集器。

+ Throughput 收集器是 Server 级虚拟机(多 CPU 的 Unix 机器以及任何 64 位虚拟机)的默 认收集器。
Throughput 收集器使用多线程回收新生代空间,Minor GC 的速度比使用 Serial 收集器快 得多。处理老年代时 Throughput 收集器也能使用多线程方式。这已经是 JDK 7u4 及之后 的版本的默认行为,对于之前老版本的 JDK 7 虚拟机,通过 -XX:+UseParallelOldGC 标 志可以开启这个功能。由于 Throughput 收集器使用多线程,Throughput 收集器也常常 被称为 Parallel 收集器。Throughput 收集器在 Minor GC 和 Full GC 时会暂停所有的应 用线程,同时在 Full GC 过程中会对老年代空间进行压缩整理。由于在大多数适用的场 景,它已经是默认的收集器,所以你基本上不需要显式地启用它。如果需要,可以使用 -XX:+UseParallelGC、-XX:+UseParallelOldGC 标志启用 Throughput 收集器。

+ CMS 收集器设计的初衷是为了消除 Throughput 收集器和 Serial 收集器 Full GC 周期中 的长时间停顿。CMS 收集器在 Minor GC 时会暂停所有的应用线程,并以多线程的方式 进行垃圾回收。然而,这其中最显著的不同是,CMS 不再使用 Throughput 的收集算法(-XX:+UseParallelGC),改用新的算法来收集新生代对象(使用 -XX:+UseParNewGC 标志)。

+ CMS 收集器在 Full GC 时不再暂停应用线程,而是使用若干个后台线程定期地对老年代空 间进行扫描,及时回收其中不再使用的对象。这种算法帮助 CMS 成为一个低延迟的收集 器:应用线程只在 Minor GC 以及后台线程扫描老年代时发生极其短暂的停顿。应用程序 线程停顿的总时长与使用 Throughput 收集器比起来短得多。
这里额外付出的代价是更高的 CPU 使用:必须有足够的 CPU 资源用于运行后台的垃圾收 集线程,在应用程序线程运行的同时扫描堆的使用情况。除此之外,后台线程不再进行任 何压缩整理的工作,这意味着堆会逐渐变得碎片化。如果 CMS 的后台线程无法获得完成 他们任务所需的 CPU 资源,或者如果堆变得过度碎片化以至于无法找到连续空间分配对 象,CMS 就蜕化到 Serial 收集器的行为:暂停所有应用线程,使用单线程回收、整理老年 代空间。这之后又恢复到并发运行,再次启动后台线程(直到下一次堆变得过度碎片化)。 通过 -XX:+UseConcMarkSweepGC、-XX:+UseParNewGC 标志(默认情况下,这两个标志都是禁 用的)可以启用 CMS 垃圾收集器。

+ G1 垃圾收集器(或者垃圾优先收集器)的设计初衷是为了尽量缩短处理超大堆(大于 4 GB)时产生的停顿。G1 收集器属于 Concurrent 收集器。 其处理新生代的方式和其他多线程收集算法类似。老年代的垃圾收集工作由后台线程完成,大多数的工 作不需要暂停应用线程。由于老年代被划分到不同的区域,G1 收集器通过将对象从一个 区域复制到另一个区域,完成对象的清理工作,这也意味着在正常的处理过程中,G1 收 集器实现了堆的压缩整理(至少是部分的整理)。因此,使用 G1 收集器的堆不大容易发生 碎片化——虽然这种问题无法避免。通过标志 -XX:+UseG1GC (默认值是关闭的)可以启动 G1 垃圾收集器。

+ G1 的设计理念使得它比 CMS 更不容易遭遇 Full GC。

+ Java 也提供了一种机制让应用程序强制进行 GC: 调用 System.gc() 方法。通常情况 下,试图通过调用这个方法显式触发 GC 都不是个好主意。调用这个方法会触发Full GC(即使 JVM 使用 CMS 或者 G1 垃圾收集器), 应用程序线程会因此而停顿相当长 的一段时间。调用这个方法也不会让应用程序更高效。

+ 在做性能监控或者基准测试时。运行少量的代码进行基准 测试时,为了更快地预热 JVM,在测量周期之前强制进行一次 GC 还是有意义的。类 似的情况还包括在进行堆分析时,通常在获取堆转储之前,强制进行一次 Full GC 是 一个不错的主意。虽然大多数抓取堆转储的方法都能进行 Full GC,也存在其他的方法 可以强制进行 Full GC:你可以通过执行 jcmd < 进程号 > GC.run,或者使用 jconsole 连接到 JVM 在内存面板上单击“进行 GC”按钮。

+ 如果你运行的程序调用的第三方代码中错误地调用了 System.gc() 方法,可以通过 JVM 参数 -XX:+DisableExplicitGC 显式地禁止这种类型的 GC;默认情况下该标志是关闭的。

+ GC 算法的选择一方面取决于应用程序的特征,另一方面取决于应用的性能目标。

+ Serial 收集器最适用于应用程序的内存使用少于 100 MB 的场景。

+ 对批量任务而言,Throughput 收集器所引入的停顿,尤其是 Full GC 的停顿是主要的顾虑。如果有额外的 CPU 处理能力(这很可能是个问题),那么对批处理程序使用 Concurrent 收集器将极大 地提升应用程序的性能。这里的关键在于我们能否提供足够的 CPU 给 Concurrent 收集器 的线程进行后台的处理工作。

+ 多个应用程序线程、多个后台 GC 线程运行于多 CPU 的 系统上。如果操作系统无法在同时运行所有应用程序线程和 GC 后台线程,那么对 CPU 的 竞争就会反映到应用程序线程的停顿上。

+ 只有一个 CPU 可用时,CPU 将一直处于忙碌状态,要么是运行应用程序线程,要么是运 行 GC 线程。这种情况下,CMS 额外的后台线程就变成了一种负担。因此，如果使用CMS和G1收集器，必须保证足够的CPU资源，否则反而会降低系统性能。

+ 存在空闲 CPU 周期时,Concurrent收集器（CMS或G1）的性能更好；否则，Concurrent收集器会急剧下降，低于Throughput收集器。

+ 如果你关 注的仅仅是平均响应时间,那么 Throughput 收集器和 Concurrent 收集器似乎差别不大,都 能满足你的要求,你可以进一步考察 CPU 的使用情况(这时 Throughput 收集器可能是更 优的选择)。如果你关注的是 90% 或者其他百分比的响应时间,那就只能通过性能测试来 了解,完成这些任务应用程序会进行多少次 Full GC,最后决定选择哪种收集器。通常CMS收集器在百分比（如99%，90%）上的响应时间值更优秀。

+ 通常情况下,Throughput 收集器的平均响应时间比 Concurrent 收集器要好,但是在 90% 响应时间或者 99% 响应时间这几项指标上, Concurrent收集器比Throughput收集器要好一些。

+ 使用 Throughput 收集器会超负荷地进行大量 Full GC 时,切换到 Concurrent 收集器通常能获得更低的响应时间。可以查看FullGC的频率和次数，以决定。

+ 一般情况下,堆空间小于 4 GB时,CMS 收集器的性能比 G1 收集器好。CMS 收集器使用的算法比 G1 更简单,因此在比较简单的环境中(譬如堆的容量很小的情况),它运行得更快。使用大型堆或巨型堆时, 由于 G1 收集器可以分割工作,通常它比 CMS 收集器表现更好。因此，对于现代大多数的应用服务器（通常堆空间均大于8G）来说，G1的表现要好于CMS。

+ 由于 CMS 收集器不对堆进行压缩整理(除非发生了耗时的 Full GC),堆的碎片化也会触 发 CMS 收集器进行 Full GC。G1 算法在处理过程中随时进行着堆的压缩整理,不过 G1 收 集器依然可能遭遇堆的碎片化问题,但是与 CMS 收集器比较起来,它的设计让它又领先 了一步。

+ 在这三种收集器（Throughput、CMS、G1）的选择时还有一些微妙的无形因素需要考虑。Throughput 收集器是 这三个收集器中年代最久远的一个,这意味着 JVM 工程师们已经花费了大量的时间精力 雕琢把玩它,它的习性也更为大家所熟知。G1 作为相对较新的一种垃圾收集算法,更容 易碰到设计时无法预期的极端情况。相对而言,G1 算法中影响性能的调优控制开关更少, 这可能是好事,也可能是坏事。直到 Java 7u4,G1 都一直被当作实验版本,它的一些调优 特性直到 Java 7u10 中才提供出来。相对于 Java 7 及之前的版本而言,G1 的性能提升主要 体现在 Java 8 中。G1 将来的工作可能会关注在如何提高它在较小的堆上相对于 CMS 的性 能优势。

+ 选择 Concurrent 收集器时,如果堆较小,推荐使用 CMS 收集器。

+ 简单粗暴 地设置一个特别大的堆也不是解决问题的方法。GC 停顿消耗的时间取决于堆的大小,如果增大堆的空间,停顿的持续时间也会变长。这种情况下,停顿的频率会变得更少,但是 它们持续的时间会让程序的整体性能变慢。

+ 如果 Full GC 时系统发生内存交换,停顿时间会以正常停顿 时间数个量级的方式增长。类似地,如果使用 Concurrent 收集器,后台线程在回收堆时, 它的速度也可能会被拖慢,因为需要等待从磁盘复制数据到内存,结果导致发生代价昂贵 的并发模式失效。因此,调整堆大小时首要的原则就是永远不要将堆的容量设置得比机器的物理内存还大, 另外,如果同一台机器上运行着多个 JVM 实例,这个原则适用于所有堆的总和。除此之 外,你还需要为 JVM 自身以及机器上其他的应用程序预留一部分的内存空间:通常情况 下,对于普通的操作系统,应该预留至少 1 G 的内存空间。

+ 